{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f91ef9a-7a90-4e3c-9ccb-884e5107495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from copy import deepcopy \n",
    "\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a50e7e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data/english_only/prompting_results_clean/with_metrics/' #experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87d5d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aggregate_metrics_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e34d6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/shared/0/projects/research-jam-summer-2024/')\n",
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ff91d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ac617985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1272, 55)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir_human = '/home/akananth/Misc/human-llm-similarity/src/annotation/annotation_output/'\n",
    "f_human = 'annotated_instances_with_metrics_orig.jsonl'\n",
    "df = pd.read_json(base_dir_human+f_human, orient='records', lines=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9f0a312c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1088, 57)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end = df[(~df['HasResponse:::[1] Yes'].isna()) | (~df['HasResponse:::[2] No'].isna())]\n",
    "end['annotated_end'] = 1 - ((~end['HasResponse:::[1] Yes'].isna()) & \n",
    "                            (~end.annotated_turn_3.isna())).astype(int)\n",
    "end['human_end'] = 1 - ((end.human_turn_3!='[no response]') & (end.human_turn_3.apply(len)>0)).astype(int)\n",
    "end.to_json(base_dir_human+re.sub('.jsonl','_end.jsonl',f_human), orient='records', lines=True)\n",
    "end.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d351d18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6292004634994206\n",
      "0.5979591836734693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(863, 57)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_sample = pd.concat([\n",
    "    end[end['human_end']==1],\n",
    "    end[end['human_end']==0].sample(int(sum(end['human_end']==1)/1.6954177897574123)),\n",
    "])\n",
    "print(np.mean(f1_sample['human_end']))\n",
    "print(f1_score(f1_sample['human_end'], f1_sample['annotated_end']))\n",
    "f1_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "91183211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>annotated_end</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human_end</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.460405</td>\n",
       "      <td>0.539595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "annotated_end         0         1\n",
       "human_end                        \n",
       "0              0.550000  0.450000\n",
       "1              0.460405  0.539595"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(f1_sample['human_end'], f1_sample['annotated_end'], normalize=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5381df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_sample[['user','instance_id','annotated_turn_3','human_turn_3','human_end','annotated_end']].\\\n",
    "to_csv('data/agg_metrics/human_baseline_end.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1384b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcfad63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "982c67d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293, 57)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir_human = '/home/akananth/Misc/human-llm-similarity/src/annotation/annotation_output/'\n",
    "f_human = 'annotated_instances_with_metrics.jsonl'\n",
    "metrics = end[(end.human_end==0) & (end.annotated_end==0)]\n",
    "metrics.to_json(base_dir_human+f_human, orient='records', lines=True)\n",
    "metrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c8347271",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.to_csv('data/agg_metrics/human_baseline_metrics.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26280b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "af4db587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotated_instances_with_metrics.jsonl\n",
      "read metrics\n",
      "read embeddings\n"
     ]
    }
   ],
   "source": [
    "base_dir_human = '/home/akananth/Misc/human-llm-similarity/src/annotation/annotation_output/'\n",
    "f_human = 'annotated_instances_with_metrics.jsonl'\n",
    "metrics = make_human_vs_llm_df(f_human, base_dir_human, read_dep = False, \n",
    "                               sim_prefix='annotated_', turn_3_prefix='')\n",
    "metrics.columns = metrics.columns.str.replace('annotated_','llm_')\n",
    "metrics = metrics[~metrics.llm_word_length.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b1d6a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log scale heavy-tailed count metrics\n",
    "for k in ['word_count', 'word_length', 'perplexity', 'dep_dpth', 'dep_brth', 'dep_dep_dist']:\n",
    "    metrics['human_'+k] = np.log(metrics['human_'+k]+1)\n",
    "    metrics['llm_'+k] = np.log(metrics['llm_'+k]+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "59365f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({True: 20})\n",
      "Counter({True: 20})\n",
      "Counter({False: 3, True: 1})\n",
      "Counter({True: 41})\n"
     ]
    }
   ],
   "source": [
    "# check that columns are exactly the merge keys and human/llm metrics from all_metrics \n",
    "print(Counter(['human_'+k in metrics.columns for k in all_metrics]))\n",
    "print(Counter(['llm_'+k in metrics.columns for k in all_metrics]))\n",
    "print(Counter([k in metrics.columns for k in merge_keys]))\n",
    "print(Counter([k in merge_keys or re.sub('human_|llm_','',k) in all_metrics \n",
    "               for k in metrics.columns]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e4a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb4c66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2d7cec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE COLUMN AGGREGATES WITH CORRELATION\n",
    "def corr_metric(corr_method, var = 'model', human_prefix='human_', sim_prefix='llm_'):\n",
    "    vals = []\n",
    "    metric = []\n",
    "    cor = []\n",
    "    for lvl in set(metrics[var]):\n",
    "        print(lvl)\n",
    "        sub = metrics[metrics[var] == lvl]\n",
    "        for k in all_metrics:\n",
    "            #print(k)\n",
    "            vals.append(lvl)\n",
    "            metric.append(k)\n",
    "            cor.append(col_diff_correlate(sub[human_prefix+k], sub[sim_prefix+k], all_metrics[k], corr_method))\n",
    "    col_corr = pd.DataFrame({var: vals, 'metric': metric, 'cor': cor, 'corr_method': corr_method})\n",
    "    col_corr['category'] = col_corr['metric'].replace(metric_category)\n",
    "    return col_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "464b1458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotated_instances_with_metrics\n",
      "annotated_instances_with_metrics\n",
      "annotated_instances_with_metrics\n"
     ]
    }
   ],
   "source": [
    "col_corr = {m: corr_metric(m, var='model') \n",
    "            for m in ['pearson','spearman','kendall']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9e1fc572",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_corr['pearson'].to_csv('data/agg_metrics/cor_metrics_human_baseline.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bf2765f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>metric</th>\n",
       "      <th>cor</th>\n",
       "      <th>corr_method</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>word_count</td>\n",
       "      <td>0.168497</td>\n",
       "      <td>pearson</td>\n",
       "      <td>lexical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>word_length</td>\n",
       "      <td>0.149804</td>\n",
       "      <td>pearson</td>\n",
       "      <td>lexical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>perplexity</td>\n",
       "      <td>0.033057</td>\n",
       "      <td>pearson</td>\n",
       "      <td>lexical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>typo</td>\n",
       "      <td>0.247590</td>\n",
       "      <td>pearson</td>\n",
       "      <td>lexical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.136363</td>\n",
       "      <td>pearson</td>\n",
       "      <td>syntactic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>dep_dpth</td>\n",
       "      <td>0.160833</td>\n",
       "      <td>pearson</td>\n",
       "      <td>syntactic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>dep_brth</td>\n",
       "      <td>0.213394</td>\n",
       "      <td>pearson</td>\n",
       "      <td>syntactic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>dep_dep_dist</td>\n",
       "      <td>0.237395</td>\n",
       "      <td>pearson</td>\n",
       "      <td>syntactic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>sbert</td>\n",
       "      <td>-0.006130</td>\n",
       "      <td>pearson</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>liwc</td>\n",
       "      <td>0.082560</td>\n",
       "      <td>pearson</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>topic</td>\n",
       "      <td>0.189567</td>\n",
       "      <td>pearson</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>0.140725</td>\n",
       "      <td>pearson</td>\n",
       "      <td>style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>capitalization</td>\n",
       "      <td>0.551310</td>\n",
       "      <td>pearson</td>\n",
       "      <td>style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.148682</td>\n",
       "      <td>pearson</td>\n",
       "      <td>style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>politeness</td>\n",
       "      <td>0.190839</td>\n",
       "      <td>pearson</td>\n",
       "      <td>style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>formality</td>\n",
       "      <td>-0.043159</td>\n",
       "      <td>pearson</td>\n",
       "      <td>style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>toxicity</td>\n",
       "      <td>0.212385</td>\n",
       "      <td>pearson</td>\n",
       "      <td>style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>readability</td>\n",
       "      <td>0.069684</td>\n",
       "      <td>pearson</td>\n",
       "      <td>style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>subjectivity</td>\n",
       "      <td>0.102488</td>\n",
       "      <td>pearson</td>\n",
       "      <td>style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>annotated_instances_with_metrics</td>\n",
       "      <td>luar</td>\n",
       "      <td>-0.003406</td>\n",
       "      <td>pearson</td>\n",
       "      <td>style</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               model          metric       cor corr_method  \\\n",
       "0   annotated_instances_with_metrics      word_count  0.168497     pearson   \n",
       "1   annotated_instances_with_metrics     word_length  0.149804     pearson   \n",
       "2   annotated_instances_with_metrics      perplexity  0.033057     pearson   \n",
       "3   annotated_instances_with_metrics            typo  0.247590     pearson   \n",
       "4   annotated_instances_with_metrics             pos  0.136363     pearson   \n",
       "5   annotated_instances_with_metrics        dep_dpth  0.160833     pearson   \n",
       "6   annotated_instances_with_metrics        dep_brth  0.213394     pearson   \n",
       "7   annotated_instances_with_metrics    dep_dep_dist  0.237395     pearson   \n",
       "8   annotated_instances_with_metrics           sbert -0.006130     pearson   \n",
       "9   annotated_instances_with_metrics            liwc  0.082560     pearson   \n",
       "10  annotated_instances_with_metrics           topic  0.189567     pearson   \n",
       "11  annotated_instances_with_metrics     punctuation  0.140725     pearson   \n",
       "12  annotated_instances_with_metrics  capitalization  0.551310     pearson   \n",
       "13  annotated_instances_with_metrics       sentiment  0.148682     pearson   \n",
       "14  annotated_instances_with_metrics      politeness  0.190839     pearson   \n",
       "15  annotated_instances_with_metrics       formality -0.043159     pearson   \n",
       "16  annotated_instances_with_metrics        toxicity  0.212385     pearson   \n",
       "17  annotated_instances_with_metrics     readability  0.069684     pearson   \n",
       "18  annotated_instances_with_metrics    subjectivity  0.102488     pearson   \n",
       "19  annotated_instances_with_metrics            luar -0.003406     pearson   \n",
       "\n",
       "     category  \n",
       "0     lexical  \n",
       "1     lexical  \n",
       "2     lexical  \n",
       "3     lexical  \n",
       "4   syntactic  \n",
       "5   syntactic  \n",
       "6   syntactic  \n",
       "7   syntactic  \n",
       "8    semantic  \n",
       "9    semantic  \n",
       "10   semantic  \n",
       "11      style  \n",
       "12      style  \n",
       "13      style  \n",
       "14      style  \n",
       "15      style  \n",
       "16      style  \n",
       "17      style  \n",
       "18      style  \n",
       "19      style  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_corr['pearson']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b555a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2053bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff417fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
